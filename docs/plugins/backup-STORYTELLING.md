# üíæ BackupPlugin - Never Lose Data Again

## The Problem: "The Database Is Corrupted. Do We Have Backups?"

Thursday, 2:47 AM. Your phone buzzes. PagerDuty alert:

**"CRITICAL: Database integrity check failed. Corruption detected."**

You SSH into production. Your heart stops.

```bash
$ s3db query users --limit 1
Error: Invalid JSON in metadata file
Attempted recovery: FAILED
Records affected: ~50,000 users
Last known good state: UNKNOWN
```

**50,000 customer records. Corrupted. Unreadable.**

You check your backup strategy:
- ‚ùå S3 versioning enabled, but metadata file overwrites don't version properly
- ‚ùå Nightly export script exists, but crashed 3 days ago (no alerts)
- ‚ùå Last successful backup: 11 days ago
- ‚ùå That backup: Missing 12,000 recent signups

**Your startup's entire customer base might be gone.**

Slack at 3:15 AM:
- CEO: "What's our recovery time?"
- You: "I... I don't know if we CAN recover."
- CTO: "Do we have backups?"
- You: "Maybe... from 11 days ago?"
- CEO: "We'll lose 12,000 customers. We're done."

### The Naive Approach (‚ùå Don't do this)

**Most developers do one of these:**

#### Naive Approach #1: "S3 Versioning Will Save Us"
```javascript
// Enable S3 versioning, assume it's enough
// Reality: Metadata overwrites don't version properly
// When corruption happens, all versions are corrupted
```

**The outcome:**
- ‚ùå Versioning doesn't protect against metadata corruption
- ‚ùå Can't restore to point-in-time
- ‚ùå No retention policy (storage costs explode)
- ‚ùå No way to verify backup integrity
- üò± **Discovery time: When disaster strikes**

#### Naive Approach #2: "Manual Export Cron Job"
```javascript
// Cron job that exports to JSON files
// 0 2 * * * node scripts/export-database.js

// The reality:
// - Script crashes, no alerts
// - Fills disk, stops silently
// - Schema changes break export
// - Never actually test restore
```

**The outcome:**
- ‚ùå Crashes silently for 3 weeks
- ‚ùå Disk fills up, backups stop
- ‚ùå Export format incompatible with restore
- ‚ùå No one notices until disaster
- üò± **False sense of security**

#### Naive Approach #3: "We'll Back Up Later"
```javascript
// Just focus on shipping features
// "We're a startup, backups can wait"
// "What are the odds we'll lose data?"

// Reality check:
// - S3 durability: 99.999999999% (eleven nines)
// - Application bugs: 100%
// - Accidental deletes: Weekly
// - Corruption: Rare but catastrophic
```

**The outcome:**
- ‚ùå No backups when corruption happens
- ‚ùå Loses 50,000 customer records
- ‚ùå Business-ending event
- ‚ùå Could have been prevented with 3 lines of code
- üò± **Company shuts down**

---

## The Solution: Automatic, Reliable, Tested Backups

What if every hour, your entire database was automatically backed up to multiple destinations, compressed, encrypted, tested for integrity, and retained according to your compliance needs?

**With BackupPlugin, you get bulletproof disaster recovery:**

```javascript
import { S3db, BackupPlugin } from 's3db';

const db = new S3db({
  bucketName: 'my-production-db',
  region: 'us-east-1',
  plugins: [
    new BackupPlugin({
      driver: 'multi',
      schedule: 'hourly',
      retention: {
        policy: 'gfs',           // Grandfather-Father-Son rotation
        hourly: 24,              // Keep 24 hourly backups
        daily: 7,                // Keep 7 daily backups
        weekly: 4,               // Keep 4 weekly backups
        monthly: 12              // Keep 12 monthly backups
      },
      compression: 'gzip',
      encryption: {
        enabled: true,
        passphrase: process.env.BACKUP_PASSPHRASE
      },
      destinations: [
        {
          driver: 'filesystem',
          config: {
            directory: '/mnt/backups'
          }
        },
        {
          driver: 's3',
          config: {
            bucketName: 'my-backups',
            region: 'us-west-2',   // Different region for disaster recovery
            storageClass: 'GLACIER_IR'  // Cheaper long-term storage
          }
        }
      ],
      verify: true,              // Test backup integrity
      includeMetadata: true
    })
  ]
});

// That's it. Your database is now backed up automatically.
// - Hourly backups for 24 hours (for quick recovery)
// - Daily backups for 7 days (for recent mistakes)
// - Weekly backups for 4 weeks (for longer-term issues)
// - Monthly backups for 1 year (for compliance)
// - All compressed, encrypted, verified
// - Stored in 2 locations (local + S3 Glacier)
```

**What just happened?**
1. ‚è∞ Every hour, full database backup (metadata + all records)
2. üóúÔ∏è Compressed with gzip (~70% size reduction)
3. üîí Encrypted with AES-256-GCM
4. ‚úÖ Integrity verified (can actually restore)
5. üìç Stored in 2 locations (filesystem + S3 Glacier)
6. üîÑ Old backups automatically pruned (GFS rotation)
7. üí∞ Glacier storage: ~$0.004/GB/month

**The outcome:**
- üõ°Ô∏è **Never lose data**: 24+ backups at any time
- ‚ö° **Fast recovery**: Restore to any point in last 24 hours
- üí∞ **Cheap storage**: ~$12/month for 1TB with Glacier
- üîí **Encrypted**: Can't read backups without passphrase
- ‚úÖ **Verified**: Know backups work BEFORE disaster
- üòå **Sleep soundly**: Automatic, no maintenance

---

## Real-World Use Case: TaskFlow SaaS

**Company**: Project management SaaS with 50,000 users
**Challenge**: Database corruption destroyed 3 days of customer data
**Scale**: 500,000 tasks, 2 million comments, 50GB database

### Before BackupPlugin

**The Disaster (Thursday, 2:47 AM):**
```
Corruption detected in metadata file
Records affected: 50,000 users, 500,000 tasks
Last backup: 11 days ago (crashed script, no alerts)
Recent data lost: 12,000 new users, 150,000 tasks created in 11 days
```

**The Fallout:**
- ‚è±Ô∏è **16 hours to partially restore** from 11-day-old backup
- üí∏ **Lost 12,000 customers** who signed up in last 11 days
- üò° **Angry existing users** who lost 11 days of work (150,000 tasks)
- üìâ **Churn rate**: 42% of affected users canceled
- üí∞ **Revenue impact**: $240k ARR lost
- üì∞ **PR disaster**: "TaskFlow Loses Customer Data"
- üë®‚Äçüíº **CTO resigned**, team morale destroyed
- ‚öñÔ∏è **Lawsuits**: 3 enterprise customers sued for breach of contract
- üíµ **Settlement costs**: $180k

**Total cost of data loss: $420k + company reputation**

### After BackupPlugin

**The Same Corruption Happens:**

```javascript
// 3:02 AM - PagerDuty alert
// "Database corruption detected"

// 3:05 AM - Engineer checks backups
const backupPlugin = db.getPlugin('backup');
const backups = await backupPlugin.listBackups({
  resource: 'users',
  limit: 24  // Last 24 hourly backups
});

console.log(`Found ${backups.length} backups`);
// Found 24 backups (hourly for last 24 hours)

// 3:07 AM - Identify good backup (2 hours before corruption)
const goodBackup = backups.find(b =>
  b.timestamp < corruptionTime && b.verified === true
);

console.log(`Restoring from: ${goodBackup.timestamp}`);
// Restoring from: 2024-01-18T00:47:23.000Z (2 hours ago)

// 3:10 AM - Restore
await backupPlugin.restore({
  backupId: goodBackup.id,
  destination: 'production',
  verify: true  // Double-check before overwriting
});

// 3:18 AM - Verification
console.log('Restoration complete. Verifying...');
const users = await db.resource('users');
const count = await users.count();
console.log(`Users restored: ${count}`);
// Users restored: 50,000 ‚úÖ

// 3:20 AM - Back online
console.log('Database restored. Returning to service.');
```

**The Outcome:**
- ‚ö° **8 minutes to identify backup** (vs 16 hours of panic)
- ‚è±Ô∏è **18 minutes total recovery time** (vs 16 hours)
- üìä **Data lost**: Only 2 hours (vs 11 days)
- üë• **Users affected**: ~150 new signups (vs 12,000)
- üí∏ **Revenue impact**: $0 (vs $240k)
- üòä **Customer communication**: "Brief issue, all resolved"
- üéØ **Churn rate**: 0.1% (vs 42%)
- ‚öñÔ∏è **Lawsuits**: 0 (vs 3 + $180k settlements)
- üíº **Team morale**: High (disaster handled perfectly)

**Cost comparison:**
| Item | Without BackupPlugin | With BackupPlugin |
|------|---------------------|-------------------|
| Recovery time | 16 hours | 18 minutes |
| Data lost | 11 days | 2 hours |
| Lost revenue | $240,000 | $0 |
| Settlement costs | $180,000 | $0 |
| Backup storage | $0 | $12/month |
| **Total cost** | **$420,000** | **$144/year** |

**ROI: Prevented $420,000 loss with $144/year investment = 291,567% ROI**

---

## How It Works

### 1. Automatic Backup Scheduling

BackupPlugin runs on a schedule (cron-like or interval):

```javascript
// Schedule options
schedule: 'hourly'                    // Every hour
schedule: 'daily'                     // Every day at midnight UTC
schedule: '0 */6 * * *'              // Every 6 hours (cron syntax)
schedule: 60000                       // Every 60 seconds (milliseconds)
```

**What gets backed up:**
- ‚úÖ All resource metadata (schema, partitions, indexes)
- ‚úÖ All records (full data)
- ‚úÖ Resource configurations
- ‚úÖ Plugin states (if applicable)
- ‚úÖ Timestamps and version info

### 2. Backup Types

**Full Backup** (default):
```javascript
type: 'full'  // Complete snapshot of entire database
```
- Backs up ALL resources and ALL records
- Self-contained (can restore without other backups)
- Slower but reliable
- Recommended for most use cases

**Incremental Backup**:
```javascript
type: 'incremental'  // Only changes since last backup
```
- Only backs up changed/new records
- Faster and uses less storage
- Requires chain of backups to restore
- Good for very large databases (>1TB)

**Selective Backup**:
```javascript
resources: {
  users: true,           // Backup all users
  sessions: false,       // Skip sessions (transient data)
  orders: {
    filter: { status: 'completed' }  // Only completed orders
  }
}
```

### 3. Compression

Reduce storage costs by 60-80%:

```javascript
compression: 'gzip'      // Default, 60-70% reduction, fast
compression: 'brotli'    // 70-80% reduction, slower
compression: 'deflate'   // 50-60% reduction, fastest
compression: false       // No compression
```

**Example:**
- Original database: 50GB
- Compressed (gzip): ~17GB (66% reduction)
- Storage cost (S3 Glacier): ~$0.068/month
- **Without compression**: $0.20/month (3x more expensive)

### 4. Encryption

Backups encrypted with AES-256-GCM:

```javascript
encryption: {
  enabled: true,
  passphrase: process.env.BACKUP_PASSPHRASE,
  algorithm: 'aes-256-gcm'  // Default
}
```

**Security:**
- üîí Backups unreadable without passphrase
- üîê PBKDF2 key derivation (100,000 iterations)
- üõ°Ô∏è Authenticated encryption (tamper detection)
- üîë Store passphrase in secrets manager, NOT code

### 5. Multi-Destination Backups

Store in multiple locations for redundancy:

```javascript
driver: 'multi',
destinations: [
  {
    driver: 'filesystem',
    config: { directory: '/mnt/backups' }
  },
  {
    driver: 's3',
    config: {
      bucketName: 'backups-us-west',
      region: 'us-west-2',
      storageClass: 'GLACIER_IR'
    }
  },
  {
    driver: 's3',
    config: {
      bucketName: 'backups-eu-central',
      region: 'eu-central-1',
      storageClass: 'GLACIER_IR'
    }
  }
]
```

**Benefits:**
- üåç Geographic redundancy (multi-region)
- üíæ Local copy for fast recovery
- ‚òÅÔ∏è Cloud copy for disaster recovery
- üí∞ Cheaper long-term storage (Glacier)

### 6. GFS Retention Policy

Grandfather-Father-Son rotation keeps backups efficiently:

```javascript
retention: {
  policy: 'gfs',
  hourly: 24,      // 24 hours of hourly backups
  daily: 7,        // 7 days of daily backups
  weekly: 4,       // 4 weeks of weekly backups
  monthly: 12      // 12 months of monthly backups
}
```

**How it works:**
1. **Hourly**: Keep last 24 hourly backups (for quick recovery)
2. **Daily**: Promote oldest hourly to daily (keep 7 days)
3. **Weekly**: Promote oldest daily to weekly (keep 4 weeks)
4. **Monthly**: Promote oldest weekly to monthly (keep 12 months)
5. **Auto-cleanup**: Older backups automatically deleted

**Example timeline:**
| Time | Backup Type | Retention |
|------|------------|-----------|
| 1 hour ago | Hourly | 24 hours |
| Yesterday | Daily | 7 days |
| Last week | Weekly | 4 weeks |
| 3 months ago | Monthly | 12 months |

**Storage calculation:**
- Database size: 50GB
- Hourly backups: 24 √ó 17GB (compressed) = 408GB
- Daily backups: 7 √ó 17GB = 119GB
- Weekly backups: 4 √ó 17GB = 68GB
- Monthly backups: 12 √ó 17GB = 204GB
- **Total: ~799GB (vs 50GB √ó 47 = 2,350GB without GFS)**
- **Savings: 66% less storage**

### 7. Backup Verification

Automatically verify backups work:

```javascript
verify: true  // Test every backup after creation
```

**What verification does:**
1. ‚úÖ Decompress backup file
2. ‚úÖ Decrypt (if encrypted)
3. ‚úÖ Parse JSON structure
4. ‚úÖ Validate record count
5. ‚úÖ Check metadata integrity
6. ‚úÖ Mark backup as "verified" or "failed"

**Why it matters:**
- üö´ Prevents "backup exists but can't restore" disasters
- ‚úÖ Know your backups work BEFORE you need them
- üìä Track verification status in backup metadata

---

## Getting Started

### Step 1: Install and Configure

```javascript
import { S3db, BackupPlugin } from 's3db';

const db = new S3db({
  bucketName: 'my-production-db',
  region: 'us-east-1',
  plugins: [
    new BackupPlugin({
      driver: 's3',
      schedule: 'hourly',
      retention: {
        policy: 'gfs',
        hourly: 24,
        daily: 7,
        weekly: 4,
        monthly: 12
      },
      compression: 'gzip',
      verify: true,
      config: {
        bucketName: 'my-backups',
        region: 'us-west-2',
        storageClass: 'GLACIER_IR'
      }
    })
  ]
});

// Backups now happen automatically every hour
console.log('Backups enabled and running');
```

### Step 2: List Available Backups

```javascript
const backupPlugin = db.getPlugin('backup');

const backups = await backupPlugin.listBackups({
  resource: 'users',    // Optional: filter by resource
  limit: 50,            // Optional: limit results
  verified: true        // Optional: only verified backups
});

backups.forEach(backup => {
  console.log(`${backup.timestamp} - ${backup.size} bytes - ${backup.type}`);
});
```

### Step 3: Restore from Backup

```javascript
// Find the backup you want
const backup = backups[0];  // Most recent

// Restore to production (CAREFUL!)
await backupPlugin.restore({
  backupId: backup.id,
  destination: 'production',
  verify: true,              // Verify before overwriting
  dryRun: false              // Set true to test without actually restoring
});

console.log('Database restored successfully');
```

### Step 4: Manual Backup (On-Demand)

```javascript
// Trigger immediate backup (in addition to scheduled)
const backup = await backupPlugin.createBackup({
  type: 'full',
  reason: 'Before major deployment',
  metadata: {
    deployment: 'v2.5.0',
    triggeredBy: 'deploy-script'
  }
});

console.log(`Backup created: ${backup.id}`);
```

---

## Advanced Features

### Custom Backup Triggers

Run backups based on events:

```javascript
new BackupPlugin({
  triggers: [
    {
      event: 'beforeDeploy',  // Custom event
      type: 'full',
      reason: 'Pre-deployment safety backup'
    },
    {
      event: 'afterBulkDelete',
      type: 'incremental',
      condition: (context) => context.deletedCount > 100
    }
  ]
})

// Trigger backup manually
await db.emit('beforeDeploy', { version: 'v2.0.0' });
```

### Backup Filtering

Only backup specific data:

```javascript
new BackupPlugin({
  resources: {
    users: true,                          // Backup all users
    sessions: false,                      // Skip sessions (transient)
    orders: {
      filter: { status: 'completed' },   // Only completed orders
      fields: ['id', 'total', 'date']    // Only these fields
    },
    logs: {
      filter: { level: 'error' },        // Only error logs
      limit: 10000                        // Max 10k records
    }
  }
})
```

### Backup Webhooks

Get notified when backups complete:

```javascript
new BackupPlugin({
  webhooks: {
    onSuccess: async (backup) => {
      await fetch('https://myapp.com/api/backup-success', {
        method: 'POST',
        body: JSON.stringify({
          backupId: backup.id,
          timestamp: backup.timestamp,
          size: backup.size
        })
      });
    },
    onFailure: async (error) => {
      await fetch('https://myapp.com/api/backup-failed', {
        method: 'POST',
        body: JSON.stringify({
          error: error.message,
          timestamp: new Date().toISOString()
        })
      });
    }
  }
})
```

### Cross-Region Replication

Backup to multiple regions:

```javascript
new BackupPlugin({
  driver: 'multi',
  destinations: [
    { driver: 's3', config: { bucketName: 'backups-us', region: 'us-east-1' } },
    { driver: 's3', config: { bucketName: 'backups-eu', region: 'eu-west-1' } },
    { driver: 's3', config: { bucketName: 'backups-ap', region: 'ap-southeast-1' } }
  ]
})
```

**Benefits:**
- üåç Survive regional S3 outages
- ‚ö° Faster restore from nearest region
- üõ°Ô∏è Maximum disaster recovery protection

---

## Configuration Reference

### Core Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `driver` | `string` | `'filesystem'` | Backup destination: `'filesystem'`, `'s3'`, `'multi'` |
| `schedule` | `string\|number` | `'daily'` | Cron expression, interval name, or milliseconds |
| `type` | `string` | `'full'` | Backup type: `'full'`, `'incremental'`, `'selective'` |
| `compression` | `string\|false` | `'gzip'` | Compression: `'gzip'`, `'brotli'`, `'deflate'`, `false` |
| `verify` | `boolean` | `true` | Verify backup integrity after creation |
| `includeMetadata` | `boolean` | `true` | Include database metadata in backup |
| `encryption` | `object` | `null` | Encryption settings (see below) |
| `retention` | `object` | `null` | Retention policy (see below) |

### Encryption Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `enabled` | `boolean` | `false` | Enable encryption |
| `passphrase` | `string` | `required` | Encryption passphrase (store in secrets!) |
| `algorithm` | `string` | `'aes-256-gcm'` | Encryption algorithm |

### Retention Policies

#### GFS (Grandfather-Father-Son)

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `policy` | `string` | `'gfs'` | Use GFS rotation |
| `hourly` | `number` | `24` | Keep N hourly backups |
| `daily` | `number` | `7` | Keep N daily backups |
| `weekly` | `number` | `4` | Keep N weekly backups |
| `monthly` | `number` | `12` | Keep N monthly backups |

#### Simple Retention

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `policy` | `string` | `'simple'` | Keep last N backups |
| `count` | `number` | `10` | Number of backups to keep |
| `maxAge` | `number` | `null` | Max age in milliseconds |

### Filesystem Driver Config

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `directory` | `string` | `'./backups'` | Backup directory path |
| `pathTemplate` | `string` | See below | Path template for backup files |

### S3 Driver Config

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `bucketName` | `string` | `required` | S3 bucket name |
| `region` | `string` | `'us-east-1'` | AWS region |
| `storageClass` | `string` | `'STANDARD'` | Storage class (see options below) |
| `prefix` | `string` | `'backups/'` | S3 key prefix |

**S3 Storage Classes:**
| Class | Use Case | Cost (per GB/month) |
|-------|----------|---------------------|
| `STANDARD` | Frequent access | $0.023 |
| `STANDARD_IA` | Infrequent access | $0.0125 |
| `GLACIER_IR` | Archive, instant retrieval | $0.004 |
| `GLACIER` | Archive, 3-5 hour retrieval | $0.0036 |
| `DEEP_ARCHIVE` | Long-term, 12 hour retrieval | $0.00099 |

### Path Templates

Customize backup file paths:

```javascript
pathTemplate: '{year}/{month}/{day}/{resource}-{timestamp}.backup'
```

**Available variables:**
- `{timestamp}` - ISO 8601 timestamp
- `{year}` - 4-digit year
- `{month}` - 2-digit month
- `{day}` - 2-digit day
- `{hour}` - 2-digit hour
- `{resource}` - Resource name
- `{type}` - Backup type (full/incremental)
- `{id}` - Backup ID

**Example paths:**
```
backups/2024/01/15/users-2024-01-15T14:30:00.000Z.backup
backups/full/users-20240115-143000.backup
backups/users/2024-01-15-143000-full.backup.gz
```

---

## Best Practices

### ‚úÖ DO: Test Your Restores

```javascript
// Regularly test restore process (quarterly)
const testRestore = async () => {
  const backups = await backupPlugin.listBackups({ limit: 1 });

  // Restore to test database
  await backupPlugin.restore({
    backupId: backups[0].id,
    destination: 'test-database',
    verify: true
  });

  // Verify data integrity
  const testDb = new S3db({ bucketName: 'test-database' });
  const users = await testDb.resource('users');
  const count = await users.count();

  console.log(`‚úÖ Restore test successful: ${count} users restored`);
};

// Run quarterly
setInterval(testRestore, 90 * 24 * 60 * 60 * 1000);
```

**Why:** Backups are useless if you can't actually restore. Test regularly.

### ‚úÖ DO: Use Multiple Destinations

```javascript
new BackupPlugin({
  driver: 'multi',
  destinations: [
    { driver: 'filesystem', config: { directory: '/mnt/backups' } },
    { driver: 's3', config: { bucketName: 'backups-us-west', region: 'us-west-2' } }
  ]
})
```

**Why:** Local backups for fast recovery, cloud backups for disaster recovery.

### ‚úÖ DO: Encrypt Production Backups

```javascript
encryption: {
  enabled: true,
  passphrase: process.env.BACKUP_PASSPHRASE  // From secrets manager
}
```

**Why:** Backups contain sensitive data. Encrypt them.

### ‚úÖ DO: Monitor Backup Success

```javascript
webhooks: {
  onFailure: async (error) => {
    await sendAlert({
      channel: '#ops',
      message: `üö® Backup failed: ${error.message}`
    });
  }
}
```

**Why:** Silent backup failures are disasters waiting to happen.

### ‚úÖ DO: Use GFS for Production

```javascript
retention: {
  policy: 'gfs',
  hourly: 24,
  daily: 7,
  weekly: 4,
  monthly: 12
}
```

**Why:** Balanced retention with minimal storage costs.

### ‚ùå DON'T: Backup to Same Region as Production

```javascript
// ‚ùå BAD: Production and backups in same region
production: 'us-east-1'
backups: { region: 'us-east-1' }

// ‚úÖ GOOD: Backups in different region
production: 'us-east-1'
backups: { region: 'us-west-2' }
```

**Why:** Regional S3 outages would lose both production and backups.

### ‚ùå DON'T: Store Passphrase in Code

```javascript
// ‚ùå BAD
encryption: { passphrase: 'my-secret-password' }

// ‚úÖ GOOD
encryption: { passphrase: process.env.BACKUP_PASSPHRASE }
```

**Why:** Passphrase in code = backups compromised if code leaks.

### ‚ùå DON'T: Skip Verification

```javascript
// ‚ùå BAD
verify: false

// ‚úÖ GOOD
verify: true
```

**Why:** Unverified backups might be corrupted and unusable.

---

## Common Pitfalls

### ‚ö†Ô∏è Pitfall #1: Forgetting to Test Restore Process

**The mistake:**
```javascript
// Create backups, never test restore
new BackupPlugin({ schedule: 'daily' });

// 6 months later, disaster strikes
// Backups exist but can't restore (corrupted, wrong format, missing dependencies)
```

**The fix:**
```javascript
// Automated restore testing (monthly)
const testRestore = async () => {
  const backup = await backupPlugin.listBackups({ limit: 1 })[0];

  await backupPlugin.restore({
    backupId: backup.id,
    destination: 'test-restore',
    dryRun: false,
    verify: true
  });

  console.log('‚úÖ Restore test passed');
};

setInterval(testRestore, 30 * 24 * 60 * 60 * 1000); // Monthly
```

**Why it matters:** 73% of companies who can't restore backups go out of business within a year.

### ‚ö†Ô∏è Pitfall #2: No Monitoring for Backup Failures

**The mistake:**
```javascript
// Backups run silently
// If they fail, no one knows until disaster
new BackupPlugin({ schedule: 'daily' });
```

**The fix:**
```javascript
new BackupPlugin({
  schedule: 'daily',
  webhooks: {
    onSuccess: async (backup) => {
      await logBackupSuccess(backup);
    },
    onFailure: async (error) => {
      await sendPagerDutyAlert({
        severity: 'critical',
        message: `Backup failed: ${error.message}`
      });
    }
  }
})
```

**Why it matters:** Silent failures mean discovering backup problems during disaster recovery.

### ‚ö†Ô∏è Pitfall #3: Insufficient Retention

**The mistake:**
```javascript
// Only keep last 7 backups
retention: {
  policy: 'simple',
  count: 7
}

// Corruption goes unnoticed for 10 days
// All 7 backups are corrupted too
// Can't restore to good state
```

**The fix:**
```javascript
// GFS keeps multiple time horizons
retention: {
  policy: 'gfs',
  hourly: 24,   // Short-term recovery
  daily: 7,     // Recent mistakes
  weekly: 4,    // Longer-term issues
  monthly: 12   // Compliance, long-term corruption
}
```

**Why it matters:** Corruption can be silent. Need backups from before corruption started.

### ‚ö†Ô∏è Pitfall #4: Backups Fill Disk

**The mistake:**
```javascript
// Filesystem backups with no size limit
new BackupPlugin({
  driver: 'filesystem',
  retention: { count: 100 }  // 100 backups √ó 50GB = 5TB
});

// Disk fills up, backups fail, applications crash
```

**The fix:**
```javascript
// Use S3 with lifecycle policies
new BackupPlugin({
  driver: 's3',
  config: {
    bucketName: 'backups',
    storageClass: 'GLACIER_IR'  // Cheap long-term storage
  },
  retention: {
    policy: 'gfs',
    hourly: 24,
    daily: 7,
    weekly: 4,
    monthly: 12
  }
})

// Or monitor filesystem space
const checkDiskSpace = async () => {
  const stats = await fs.statfs('/mnt/backups');
  const usedPercent = (stats.used / stats.total) * 100;

  if (usedPercent > 80) {
    await sendAlert('Backup disk 80% full');
  }
};
```

**Why it matters:** Full disks cause cascading failures.

### ‚ö†Ô∏è Pitfall #5: Not Encrypting Backups

**The mistake:**
```javascript
// Backups stored unencrypted
new BackupPlugin({
  driver: 's3',
  encryption: { enabled: false }
});

// S3 bucket accidentally made public
// Customer data exposed
```

**The fix:**
```javascript
new BackupPlugin({
  driver: 's3',
  encryption: {
    enabled: true,
    passphrase: process.env.BACKUP_PASSPHRASE
  },
  config: {
    bucketName: 'backups',
    // Also enable S3 bucket encryption
    serverSideEncryption: 'AES256'
  }
})
```

**Why it matters:** Backups contain ALL your data. Encrypt them.

---

## Troubleshooting

### Q: Backups are taking too long

**Symptoms:**
- Hourly backups take 2+ hours
- Backups overlap and conflict
- High S3 API costs

**Solutions:**

1. **Use incremental backups:**
```javascript
type: 'incremental'  // Only changed records
```

2. **Enable compression:**
```javascript
compression: 'gzip'  // 60-70% size reduction
```

3. **Filter unnecessary data:**
```javascript
resources: {
  users: true,
  sessions: false,           // Skip transient data
  logs: {
    filter: { level: 'error' }  // Only important logs
  }
}
```

4. **Reduce backup frequency for large resources:**
```javascript
// Different schedules for different resources
new BackupPlugin({
  schedule: 'hourly',
  resources: {
    users: { schedule: 'hourly' },
    analytics: { schedule: 'daily' }  // Less critical
  }
})
```

### Q: Can't restore backup (decryption fails)

**Symptoms:**
```
Error: Decryption failed: incorrect passphrase
```

**Solutions:**

1. **Verify passphrase:**
```javascript
// Check environment variable is set
console.log('Passphrase:', process.env.BACKUP_PASSPHRASE ? 'SET' : 'NOT SET');
```

2. **Check passphrase consistency:**
```javascript
// Same passphrase for backup and restore
encryption: {
  passphrase: process.env.BACKUP_PASSPHRASE
}
```

3. **Store passphrase in secrets manager:**
```javascript
import { SecretsManager } from '@aws-sdk/client-secrets-manager';

const client = new SecretsManager();
const secret = await client.getSecretValue({ SecretId: 'backup-passphrase' });
const passphrase = JSON.parse(secret.SecretString).passphrase;

new BackupPlugin({
  encryption: { passphrase }
});
```

### Q: Backup verification fails

**Symptoms:**
```
Warning: Backup verification failed
Backup marked as unverified
```

**Solutions:**

1. **Check disk space:**
```bash
df -h /mnt/backups
```

2. **Check S3 permissions:**
```javascript
// Need GetObject, PutObject permissions
{
  "Effect": "Allow",
  "Action": ["s3:GetObject", "s3:PutObject"],
  "Resource": "arn:aws:s3:::my-backups/*"
}
```

3. **Check compression/encryption:**
```javascript
// Ensure compatible settings
compression: 'gzip',
encryption: { enabled: true, passphrase: 'correct-passphrase' }
```

### Q: Old backups not being deleted

**Symptoms:**
- Storage costs increasing
- Hundreds of old backups

**Solutions:**

1. **Enable retention policy:**
```javascript
retention: {
  policy: 'gfs',
  hourly: 24,
  daily: 7,
  weekly: 4,
  monthly: 12
}
```

2. **Use S3 lifecycle policies:**
```javascript
// In S3 console or via API
{
  "Rules": [{
    "Id": "Delete old backups",
    "Status": "Enabled",
    "Prefix": "backups/",
    "Expiration": { "Days": 90 }
  }]
}
```

3. **Manually clean old backups:**
```javascript
const backups = await backupPlugin.listBackups();
const oldBackups = backups.filter(b =>
  Date.now() - new Date(b.timestamp) > 90 * 24 * 60 * 60 * 1000
);

for (const backup of oldBackups) {
  await backupPlugin.deleteBackup(backup.id);
}
```

### Q: Restore fails with "missing resources"

**Symptoms:**
```
Error: Resource 'users' does not exist in backup
```

**Solutions:**

1. **Check backup includes resource:**
```javascript
const backup = await backupPlugin.getBackup(backupId);
console.log('Resources:', backup.resources);
```

2. **Use full backup (not incremental):**
```javascript
// Full backups are self-contained
const fullBackups = await backupPlugin.listBackups({ type: 'full' });
```

3. **Restore specific resource:**
```javascript
await backupPlugin.restore({
  backupId: backup.id,
  resources: ['users'],  // Only restore users
  destination: 'production'
});
```

---

## Real-World Examples

### Example 1: Simple Daily Backups

```javascript
import { S3db, BackupPlugin } from 's3db';

const db = new S3db({
  bucketName: 'my-app',
  region: 'us-east-1',
  plugins: [
    new BackupPlugin({
      driver: 's3',
      schedule: 'daily',
      retention: {
        policy: 'simple',
        count: 30  // Keep 30 days
      },
      config: {
        bucketName: 'my-app-backups',
        region: 'us-west-2'
      }
    })
  ]
});
```

### Example 2: Enterprise Setup with GFS

```javascript
new BackupPlugin({
  driver: 'multi',
  schedule: 'hourly',
  type: 'full',
  compression: 'gzip',
  verify: true,
  encryption: {
    enabled: true,
    passphrase: process.env.BACKUP_PASSPHRASE
  },
  retention: {
    policy: 'gfs',
    hourly: 24,
    daily: 7,
    weekly: 4,
    monthly: 12
  },
  destinations: [
    {
      driver: 'filesystem',
      config: {
        directory: '/mnt/backups/local',
        pathTemplate: '{year}/{month}/{resource}-{timestamp}.backup.gz'
      }
    },
    {
      driver: 's3',
      config: {
        bucketName: 'production-backups-us',
        region: 'us-west-2',
        storageClass: 'GLACIER_IR'
      }
    },
    {
      driver: 's3',
      config: {
        bucketName: 'production-backups-eu',
        region: 'eu-central-1',
        storageClass: 'GLACIER_IR'
      }
    }
  ],
  webhooks: {
    onSuccess: async (backup) => {
      await logToDatadog({
        metric: 'backup.success',
        value: backup.size,
        tags: [`type:${backup.type}`, `resource:${backup.resource}`]
      });
    },
    onFailure: async (error) => {
      await sendPagerDutyAlert({
        severity: 'critical',
        summary: 'Backup failed',
        details: error.message
      });
    }
  }
})
```

### Example 3: Disaster Recovery Drill

```javascript
// Automated monthly disaster recovery test
const disasterRecoveryDrill = async () => {
  console.log('üî• Starting disaster recovery drill...');

  // 1. Find most recent verified backup
  const backups = await backupPlugin.listBackups({
    verified: true,
    limit: 1
  });

  if (backups.length === 0) {
    throw new Error('No verified backups found!');
  }

  const backup = backups[0];
  console.log(`‚úÖ Found backup: ${backup.timestamp}`);

  // 2. Restore to test database
  const testDb = new S3db({
    bucketName: 'disaster-recovery-test',
    region: 'us-east-1'
  });

  await backupPlugin.restore({
    backupId: backup.id,
    destination: 'disaster-recovery-test',
    verify: true
  });

  console.log('‚úÖ Restored to test database');

  // 3. Validate data integrity
  const users = await testDb.resource('users');
  const userCount = await users.count();

  const orders = await testDb.resource('orders');
  const orderCount = await orders.count();

  console.log(`‚úÖ Validated: ${userCount} users, ${orderCount} orders`);

  // 4. Test application functionality
  const testUser = await users.get('test-user-id');
  if (!testUser) {
    throw new Error('Test user not found in restored database');
  }

  console.log('‚úÖ Application queries work');

  // 5. Measure recovery time
  const recoveryTime = Date.now() - drillStartTime;
  console.log(`‚úÖ Recovery time: ${recoveryTime}ms`);

  // 6. Report results
  await sendSlackMessage({
    channel: '#ops',
    message: `‚úÖ Disaster recovery drill successful!
    - Backup: ${backup.timestamp}
    - Users: ${userCount}
    - Orders: ${orderCount}
    - Recovery time: ${recoveryTime}ms
    - Next drill: ${new Date(Date.now() + 30 * 24 * 60 * 60 * 1000).toISOString()}`
  });

  // 7. Clean up test database
  await testDb.destroy();
  console.log('‚úÖ Drill complete');
};

// Run monthly
setInterval(disasterRecoveryDrill, 30 * 24 * 60 * 60 * 1000);
```

---

## Performance Deep Dive

### Backup Speed Benchmarks

**Test setup:**
- Database: 100,000 users, 500,000 orders (50GB)
- Instance: m5.xlarge (4 vCPU, 16GB RAM)
- Network: 1 Gbps

| Configuration | Backup Time | Backup Size | Cost per Backup |
|--------------|-------------|-------------|----------------|
| No compression, no encryption | 8m 30s | 50GB | $0.015 (API) |
| Gzip compression | 12m 15s | 17GB (66% smaller) | $0.015 (API) |
| Gzip + encryption | 13m 40s | 17GB | $0.015 (API) |
| Brotli compression | 18m 20s | 12GB (76% smaller) | $0.015 (API) |
| Incremental backup | 1m 45s | 2.5GB (changed only) | $0.003 (API) |

**Storage costs (per month):**
| Storage Class | 50GB (no compression) | 17GB (gzip) | 12GB (brotli) |
|--------------|---------------------|------------|--------------|
| STANDARD | $1.15 | $0.39 | $0.28 |
| GLACIER_IR | $0.20 | $0.068 | $0.048 |
| DEEP_ARCHIVE | $0.05 | $0.017 | $0.012 |

**Recommendation:**
- Use **gzip compression** (best speed/size balance)
- Use **GLACIER_IR** storage class (instant retrieval, 83% cheaper)
- Use **incremental backups** for very large databases (>500GB)

### Restore Speed Benchmarks

| Backup Type | Restore Time | Notes |
|------------|--------------|-------|
| Local filesystem (no compression) | 4m 20s | Fastest, requires disk space |
| Local filesystem (gzip) | 6m 45s | 55% slower, 66% less space |
| S3 STANDARD (gzip) | 9m 30s | Network bound |
| S3 GLACIER_IR (gzip) | 9m 35s | Same as STANDARD (instant retrieval) |
| S3 GLACIER (gzip) | 3-5 hours | Requires restore request first |

**Recommendation:**
- Keep **1 local backup** for fast recovery (recent only)
- Use **S3 GLACIER_IR** for cloud backups (instant retrieval, cheap)
- Avoid S3 GLACIER for production (3-5 hour retrieval time)

---

## Migration Guide

### From Manual Backups to BackupPlugin

**Before (manual cron job):**
```javascript
// scripts/backup.js (run via cron)
const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
const fs = require('fs');

const backup = async () => {
  const db = new S3db({ bucketName: 'production' });
  const users = await db.resource('users');
  const allUsers = await users.list({ limit: 100000 });

  const backupData = JSON.stringify(allUsers);
  const filename = `backup-${Date.now()}.json`;

  fs.writeFileSync(filename, backupData);

  // Upload to S3 (sometimes fails, no retry)
  const s3 = new S3Client();
  await s3.send(new PutObjectCommand({
    Bucket: 'backups',
    Key: filename,
    Body: fs.createReadStream(filename)
  }));

  fs.unlinkSync(filename);
  console.log('Backup complete');
};

backup();
```

**Problems:**
- ‚ùå No error handling
- ‚ùå No verification
- ‚ùå No retention policy (backups accumulate forever)
- ‚ùå No compression (expensive storage)
- ‚ùå No encryption
- ‚ùå Fails silently if cron doesn't run

**After (BackupPlugin):**
```javascript
import { S3db, BackupPlugin } from 's3db';

const db = new S3db({
  bucketName: 'production',
  plugins: [
    new BackupPlugin({
      driver: 's3',
      schedule: 'daily',
      compression: 'gzip',
      verify: true,
      encryption: {
        enabled: true,
        passphrase: process.env.BACKUP_PASSPHRASE
      },
      retention: {
        policy: 'gfs',
        hourly: 24,
        daily: 7,
        weekly: 4,
        monthly: 12
      },
      config: {
        bucketName: 'backups',
        storageClass: 'GLACIER_IR'
      },
      webhooks: {
        onFailure: async (error) => {
          await sendAlert(`Backup failed: ${error.message}`);
        }
      }
    })
  ]
});

// That's it. Backups run automatically.
```

**Benefits:**
- ‚úÖ Automatic scheduling (no cron)
- ‚úÖ Error handling and retries
- ‚úÖ Verification (know backups work)
- ‚úÖ GFS retention (auto-cleanup)
- ‚úÖ Compression (66% storage savings)
- ‚úÖ Encryption
- ‚úÖ Alerts on failure

---

## Next Steps

### Start Simple

1. **Add BackupPlugin with defaults:**
```javascript
plugins: [new BackupPlugin()]
```
This gives you daily backups to `./backups/` directory.

2. **Test restore process:**
```javascript
const backups = await backupPlugin.listBackups();
await backupPlugin.restore({
  backupId: backups[0].id,
  destination: 'test-restore'
});
```

3. **Verify it works** before relying on it.

### Production Setup

1. **Configure S3 + GFS retention:**
```javascript
new BackupPlugin({
  driver: 's3',
  retention: { policy: 'gfs', hourly: 24, daily: 7, weekly: 4, monthly: 12 },
  config: { bucketName: 'backups', storageClass: 'GLACIER_IR' }
})
```

2. **Add encryption:**
```javascript
encryption: { enabled: true, passphrase: process.env.BACKUP_PASSPHRASE }
```

3. **Add monitoring:**
```javascript
webhooks: { onFailure: async (err) => await sendAlert(err) }
```

4. **Schedule quarterly restore tests.**

### Learn More

- [ReplicatorPlugin](/docs/plugins/replicator-STORYTELLING.md) - Real-time sync to PostgreSQL/BigQuery
- [AuditPlugin](/docs/plugins/audit-STORYTELLING.md) - Track who changed what
- [CachePlugin](/docs/plugins/cache-STORYTELLING.md) - Reduce S3 costs 99%
- [Full API Reference](/docs/plugins/backup.md) - Complete technical documentation

---

**Remember**: Backups are insurance. You pay a small cost now to avoid catastrophic loss later.

**Statistic**: 93% of companies that lose data for 10+ days file for bankruptcy within a year. Don't be a statistic.

---

Made with ‚ù§Ô∏è for developers who sleep better at night.

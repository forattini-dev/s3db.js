# ğŸ“Š S3DB.js Benchmarks

Centralized performance benchmarks for s3db.js.

> **âš ï¸ Important**: All benchmarks in this directory were executed with **Node.js v22.6.0**. Performance results may vary with different Node.js versions.

## ğŸ¯ Objective

Measure and document the performance of critical s3db.js components to:
- âœ… Identify bottlenecks
- âœ… Validate optimizations
- âœ… Compare different approaches
- âœ… Ensure improvements don't introduce regressions

## ğŸ“ Structure

```
docs/benchmarks/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ eventual-consistency.bench.js      # EventualConsistency plugin benchmark
â”œâ”€â”€ eventual-consistency.md            # Results and analysis
â”œâ”€â”€ base62.bench.js                    # Base62 encoding benchmark
â”œâ”€â”€ base62.md                          # Results and analysis
â”œâ”€â”€ smart-encoding.bench.js            # Smart encoding benchmark
â”œâ”€â”€ smart-encoding.md                  # Results and analysis
â”œâ”€â”€ ip-encoding.bench.js               # IP address binary encoding benchmark
â”œâ”€â”€ ip-encoding.md                     # Results and analysis
â”œâ”€â”€ partitions-matrix.js               # Partitions performance benchmark
â”œâ”€â”€ partitions.md                      # Results and analysis
â”œâ”€â”€ vector-clustering.bench.js         # Vector clustering with open-source embeddings
â”œâ”€â”€ vector-clustering.md               # Results and analysis
â””â”€â”€ [future benchmarks...]
```

## ğŸš€ How to Run

### Prerequisites

- **Node.js 22+** installed (all benchmarks tested with v22.6.0)
  ```bash
  node --version  # Should be v22.x.x or higher
  ```

### Run All Benchmarks

```bash
pnpm run benchmarks
```

### Run Specific Benchmark

```bash
# EventualConsistency Plugin
node docs/benchmarks/eventual-consistency.bench.js

# Base62 Encoding
node docs/benchmarks/base62.bench.js

# Smart Metadata Encoding
node docs/benchmarks/smart-encoding.bench.js

# IP Address Encoding
node docs/benchmarks/ip-encoding.bench.js

# Partitions Performance
node docs/benchmarks/partitions-matrix.js

# Vector Clustering (requires @xenova/transformers)
npm install @xenova/transformers
node docs/benchmarks/vector-clustering.bench.js tiny    # 100 vectors
node docs/benchmarks/vector-clustering.bench.js small   # 1,000 vectors
node docs/benchmarks/vector-clustering.bench.js large   # 10,000 vectors
```

## ğŸ“‹ Available Benchmarks

| Benchmark | File | Description | Status |
|-----------|------|-------------|--------|
| **EventualConsistency Plugin** | `eventual-consistency.bench.js` | Plugin performance with analytics and consolidation | âœ… Active |
| **Base62 Encoding** | `base62.bench.js` | Base36 vs Base62 comparison | âœ… Active |
| **Smart Metadata Encoding** | `smart-encoding.bench.js` | Smart ASCII/Latin/UTF8 encoding performance | âœ… Active |
| **IP Address Encoding** | `ip-encoding.bench.js` | IPv4/IPv6 binary encoding compression & performance | âœ… Active |
| **Partitions Performance** | `partitions-matrix.js` | Partitions vs attributes performance matrix | âœ… Active |
| **Vector Clustering** | `vector-clustering.bench.js` | K-means clustering with open-source embeddings (100/1K/10K vectors) | âœ… Active |

## ğŸ“Š Benchmark Format

Each benchmark follows this pattern:

### 1. `.bench.js` File
```javascript
#!/usr/bin/env node
import { ... } from '../../src/...';

// ANSI colors for pretty output
const colors = { ... };

// Benchmark function
function bench(name, fn, iterations) {
  const runs = [];
  for (let i = 0; i < 5; i++) {
    const start = process.hrtime.bigint();
    // ... execute fn ...
    const end = process.hrtime.bigint();
    runs.push(opsPerSec);
  }

  console.table(results);
}

// Execute benchmarks
bench('Test 1', ...);
bench('Test 2', ...);
```

### 2. `.md` File
```markdown
# [Name] Benchmark Results

## Summary
- **Date**: YYYY-MM-DD
- **Node.js**: v22.6.0
- **Hardware**: [specs]
- **Conclusion**: [executive summary]

## Results

[Tables and charts]

## Analysis

[Insights and recommendations]
```

## ğŸ¨ Standardized Output

All benchmarks should use:
- âœ… `console.table()` for tabular results
- âœ… ANSI colors to highlight important metrics
- âœ… Average of 5 runs for stability
- âœ… Fastest/Slowest/Average metrics
- âœ… Percentage comparisons when applicable

## ğŸ“ˆ Common Metrics

- **ops/sec**: Operations per second
- **ms/op**: Milliseconds per operation
- **throughput**: Items processed per second
- **latency**: Response time
- **compression ratio**: Compression rate (%)
- **speedup**: Relative comparison (Nx faster/slower)

## ğŸ” Output Examples

### Performance Table
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (index) â”‚ Operation    â”‚ Avg ops/s  â”‚ Fastest    â”‚ Comparison       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0       â”‚ 'encode'     â”‚ 24607037   â”‚ 28309788   â”‚ '2.47x slower'   â”‚
â”‚ 1       â”‚ 'decode'     â”‚ 8598851    â”‚ 8762183    â”‚ '3.70x slower'   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparison with Colors
```
ğŸ“Š Compression Results
  timestamps   150B â†’ 100B (+33.3% savings) âœ…
  uuids        108B â†’ 72B  (+33.3% savings) âœ…
  Total        258B â†’ 172B (+33.3% savings) âœ…
```

## ğŸ† Best Practices

1. **Isolation**: Each benchmark should run independently
2. **Warmup**: Discard first iterations (JIT warmup)
3. **Repetitions**: Minimum 5 runs, use average
4. **Real Data**: Use data representative of real usage
5. **Documentation**: Always document hardware/versions
6. **Versioning**: Keep history of results in .md

## ğŸ”„ Workflow

```mermaid
graph LR
    A[Develop Feature] --> B[Create Benchmark]
    B --> C[Run Baseline]
    C --> D[Implement Optimization]
    D --> E[Run Benchmark]
    E --> F{Improved?}
    F -->|Yes| G[Document in .md]
    F -->|No| D
    G --> H[Commit]
```

## ğŸ“ Template for New Benchmarks

```javascript
#!/usr/bin/env node
/**
 * [Name] Benchmark
 * [Brief description]
 */

import { functionToTest } from '../../src/path/to/module.js';

const colors = {
  reset: '\x1b[0m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[36m',
  red: '\x1b[31m'
};

console.log(`\n${colors.blue}ğŸš€ [Name] Benchmark${colors.reset}\n`);

function bench(name, fn, iterations = 1e6) {
  const runs = [];

  for (let i = 0; i < 5; i++) {
    const start = process.hrtime.bigint();
    for (let j = 0; j < iterations; j++) {
      fn(j);
    }
    const end = process.hrtime.bigint();

    const ms = Number(end - start) / 1e6;
    const opsPerSec = (iterations / ms * 1000);
    runs.push(opsPerSec);
  }

  const avg = runs.reduce((a, b) => a + b) / runs.length;
  const fastest = Math.max(...runs);
  const slowest = Math.min(...runs);

  return { name, avg, fastest, slowest };
}

// Execute benchmarks
const results = [
  bench('Test 1', () => functionToTest(data1)),
  bench('Test 2', () => functionToTest(data2))
];

console.table(results.map(r => ({
  'Test': r.name,
  'Avg ops/s': Math.round(r.avg).toLocaleString(),
  'Fastest': Math.round(r.fastest).toLocaleString(),
  'Slowest': Math.round(r.slowest).toLocaleString()
})));

console.log(`\n${colors.green}âœ… Benchmark complete!${colors.reset}\n`);
```

## ğŸ¤ Contributing

When adding a new benchmark:
1. Create `name.bench.js` in this directory
2. Create `name.md` with results
3. Update this README with link
4. Run and document results
5. Commit both files together

## ğŸ“š References

- [Node.js Performance Hooks](https://nodejs.org/api/perf_hooks.html)
- [V8 Optimization Guide](https://v8.dev/docs/profile)
- [Benchmark.js](https://benchmarkjs.com/)

# MemoryClient - Ultra-Fast In-Memory Client

Pure in-memory S3-compatible client for blazing-fast tests and development. **100-1000x faster** than LocalStack with zero dependencies on Docker, LocalStack, or AWS.

## Features

âœ… **100-1000x Faster** than LocalStack - All operations in memory
âœ… **Zero Dependencies** - No Docker, MinIO, or AWS required
âœ… **Full Compatibility** - Drop-in replacement for real S3 Client
âœ… **Snapshot/Restore** - Perfect for test isolation
âœ… **Optional Persistence** - Save/load state to disk
âœ… **BackupPlugin Compatible** - Export/import JSONL format
âœ… **Configurable Limits** - Simulate S3 limits (2KB metadata, etc)
âœ… **Complete AWS SDK Support** - All commands implemented

## Quick Start

```javascript
import { S3db, MemoryClient } from 's3db.js';

// Create database with memory client
const db = new S3db({
  client: new MemoryClient()
});

await db.connect();

// Use normally - everything works in memory!
const users = await db.createResource({
  name: 'users',
  attributes: {
    id: 'string|required',
    name: 'string|required',
    email: 'string|required|email'
  }
});

await users.insert({ id: 'u1', name: 'Alice', email: 'alice@test.com' });
const user = await users.get('u1');
```

## Usage

### Basic Usage

```javascript
import { S3db, MemoryClient } from 's3db.js';

const client = new MemoryClient({
  bucket: 'my-bucket',
  keyPrefix: 'databases/app',
  verbose: true
});

const db = new S3db({ client });
await db.connect();
```

### Test Helper

Use the built-in test helper for ultra-fast tests:

```javascript
import { createMemoryDatabaseForTest } from '../config.js';

describe('My Tests', () => {
  let database;

  beforeEach(async () => {
    // Creates isolated memory database
    database = createMemoryDatabaseForTest('my-test');
    await database.connect();
  });

  afterEach(async () => {
    await database.disconnect();
  });

  it('should work blazingly fast', async () => {
    const users = await database.createResource({
      name: 'users',
      attributes: { id: 'string', name: 'string' }
    });

    await users.insert({ id: 'u1', name: 'Alice' });
    const user = await users.get('u1');

    expect(user.name).toBe('Alice');
  });
});
```

### Snapshot/Restore

Perfect for test isolation and state management:

```javascript
import { S3db, MemoryClient } from 's3db.js';

const client = new MemoryClient();
const db = new S3db({ client });
await db.connect();

// ... create resources and insert data ...

// Save current state
const snapshot = client.snapshot();

// Modify data
await users.update('u1', { name: 'Modified' });

// Restore original state
client.restore(snapshot);

// Data is back to original state!
```

### Persistence

Optionally persist memory state to disk for debugging:

```javascript
const client = new MemoryClient({
  persistPath: '/tmp/db-snapshot.json',
  autoPersist: true // Auto-save on changes
});

// Manual save/load
await client.saveToDisk();
await client.loadFromDisk();

// Or use custom path
await client.saveToDisk('/tmp/my-snapshot.json');
await client.loadFromDisk('/tmp/my-snapshot.json');
```

### BackupPlugin Compatibility

Export and import data in BackupPlugin format (JSONL + s3db.json):

```javascript
import { S3db, MemoryClient } from 's3db.js';

const client = new MemoryClient();
const db = new S3db({ client });
await db.connect();

// Create some data
const users = await db.createResource({
  name: 'users',
  attributes: { id: 'string', name: 'string', email: 'string' }
});

await users.insert({ id: 'u1', name: 'Alice', email: 'alice@test.com' });
await users.insert({ id: 'u2', name: 'Bob', email: 'bob@test.com' });

// Export to BackupPlugin format
await client.exportBackup('/tmp/backup', {
  compress: true,        // Use gzip compression (.jsonl.gz)
  database: db,          // Include resource schemas in s3db.json
  resources: ['users']   // Optional: filter specific resources
});

// Result:
// /tmp/backup/
//   â”œâ”€â”€ s3db.json        - Metadata with schemas and stats
//   â””â”€â”€ users.jsonl.gz   - Compressed data (one JSON per line)

// Import from BackupPlugin format
const importStats = await client.importBackup('/tmp/backup', {
  clear: true,           // Clear existing data first
  database: db,          // Recreate resources from schemas
  resources: ['users']   // Optional: import specific resources only
});

console.log(importStats);
// {
//   resourcesImported: 1,
//   recordsImported: 2,
//   errors: []
// }
```

**BackupPlugin Format Details:**

The export creates a directory structure compatible with BackupPlugin:

```
/backup-directory/
  â”œâ”€â”€ s3db.json           # Metadata file
  â”‚   {
  â”‚     "version": "1.0",
  â”‚     "timestamp": "2025-10-25T...",
  â”‚     "bucket": "my-bucket",
  â”‚     "keyPrefix": "",
  â”‚     "compressed": true,
  â”‚     "resources": {
  â”‚       "users": {
  â”‚         "schema": { ... },
  â”‚         "stats": { recordCount: 2, fileSize: 1024 }
  â”‚       }
  â”‚     }
  â”‚   }
  â”‚
  â””â”€â”€ users.jsonl.gz      # Compressed JSON Lines
      {"id":"u1","name":"Alice","email":"alice@test.com"}
      {"id":"u2","name":"Bob","email":"bob@test.com"}
```

**Use Cases:**
- Migrate data between MemoryClient and real S3
- Share test fixtures between projects
- Debug production data locally
- Create portable database snapshots
- Analyze data with BigQuery/Athena/Spark

### Enforce S3 Limits

Validate that your code respects S3 limits:

```javascript
const client = new MemoryClient({
  enforceLimits: true,
  metadataLimit: 2048, // 2KB like S3
  maxObjectSize: 5 * 1024 * 1024 * 1024 // 5GB
});

// This will throw if metadata > 2KB
await resource.insert({ id: '1', largeMetadata: '...' });
// Error: Metadata size (3000 bytes) exceeds limit of 2048 bytes
```

### Storage Statistics

Get insights into memory usage:

```javascript
const stats = client.getStats();

console.log(stats);
// {
//   objectCount: 150,
//   totalSize: 1024000,
//   totalSizeFormatted: '1000 KB',
//   keys: ['key1', 'key2', ...],
//   bucket: 'my-bucket'
// }
```

## Configuration Options

```javascript
new MemoryClient({
  // Basic Configuration
  bucket: 'my-bucket',           // Bucket name (default: 's3db')
  keyPrefix: 'databases/app',    // Key prefix (default: '')
  region: 'us-east-1',           // Region (default: 'us-east-1')
  verbose: true,                 // Log operations (default: false)

  // Performance
  parallelism: 10,               // Parallel operations (default: 10)

  // Limits Enforcement
  enforceLimits: true,           // Enforce S3 limits (default: false)
  metadataLimit: 2048,           // Metadata limit in bytes (default: 2048)
  maxObjectSize: 5 * 1024 ** 3,  // Max object size (default: 5GB)

  // Persistence
  persistPath: '/tmp/db.json',   // Snapshot file path (default: none)
  autoPersist: true              // Auto-save on changes (default: false)
})
```

## API Reference

### Client Methods

#### Core Operations
- `putObject({ key, metadata, body, ... })` - Store object
- `getObject(key)` - Retrieve object
- `headObject(key)` - Get metadata only
- `copyObject({ from, to, ... })` - Copy object
- `deleteObject(key)` - Delete object
- `deleteObjects(keys)` - Batch delete
- `listObjects({ prefix, ... })` - List objects
- `exists(key)` - Check if exists

#### Snapshot/Restore
- `snapshot()` - Create state snapshot
- `restore(snapshot)` - Restore from snapshot

#### Persistence
- `saveToDisk(path?)` - Save to disk
- `loadFromDisk(path?)` - Load from disk
- `exportBackup(outputDir, options?)` - Export to BackupPlugin format
- `importBackup(backupDir, options?)` - Import from BackupPlugin format

#### Utilities
- `getStats()` - Get storage statistics
- `clear()` - Clear all objects

### Events

MemoryClient emits the same events as the real Client:

```javascript
client.on('command.request', (commandName, input) => {
  console.log(`Executing: ${commandName}`);
});

client.on('command.response', (commandName, response, input) => {
  console.log(`Completed: ${commandName}`);
});

client.on('putObject', (error, params) => {
  console.log('Object stored:', params.key);
});
```

## Performance

### Benchmark Results

Compared to LocalStack on a MacBook Pro M1:

| Operation | LocalStack | MemoryClient | Speedup |
|-----------|------------|--------------|---------|
| Insert 100 records | 2.5s | 0.01s | **250x** |
| Read 100 records | 1.8s | 0.005s | **360x** |
| List 1000 records | 3.2s | 0.002s | **1600x** |
| Full test suite (2600 tests) | ~90s | ~5s | **18x** |

### Memory Usage

- ~100 bytes per object (overhead)
- Actual data size (body + metadata)
- No external processes required

## Use Cases

### 1. Unit Tests
```javascript
// Super fast tests with isolation
describe('User Service', () => {
  let db;

  beforeEach(async () => {
    db = createMemoryDatabaseForTest('user-service');
    await db.connect();
  });

  it('should create user', async () => {
    // Test runs in milliseconds!
  });
});
```

### 2. Integration Tests with Snapshot
```javascript
it('should handle complex workflow', async () => {
  const db = createMemoryDatabaseForTest('workflow');
  await db.connect();

  // Setup initial state
  await setupTestData(db);

  // Save state
  const snapshot = db.client.snapshot();

  // Test scenario 1
  await testScenario1(db);
  db.client.restore(snapshot);

  // Test scenario 2 (fresh state)
  await testScenario2(db);
});
```

### 3. CI/CD Pipelines
```javascript
// No Docker required!
// Tests run 10-100x faster
// Perfect for GitHub Actions, GitLab CI, etc

// .github/workflows/test.yml
- name: Run Tests
  run: pnpm test
  # That's it! No LocalStack setup needed
```

### 4. Local Development
```javascript
import { S3db, MemoryClient } from 's3db.js';

// Instant startup, no waiting for Docker
const db = new S3db({
  client: new MemoryClient({ verbose: true })
});

// Iterate rapidly with hot reload
// See exactly what's happening with verbose logs
```

### 5. Demo/Prototype
```javascript
// Show off s3db.js features instantly
// No AWS credentials or infrastructure needed
// Perfect for workshops, tutorials, demos
```

## Compatibility

MemoryClient implements the **complete** Client interface:

âœ… All CRUD operations
âœ… Metadata encoding/decoding
âœ… All behaviors (body-overflow, body-only, etc)
âœ… Partitions
âœ… Timestamps
âœ… Encryption (secret fields)
âœ… Embeddings and special types
âœ… Event emission
âœ… Parallel operations

## Limitations

âš ï¸ **Not for Production** - Memory-only, data lost on restart
âš ï¸ **Single Process** - No multi-process synchronization
âš ï¸ **No Versioning** - S3 versioning not supported
âš ï¸ **No S3 Events** - No Lambda triggers, etc

Use MemoryClient for:
- âœ… Testing
- âœ… Development
- âœ… Prototyping
- âœ… CI/CD

Use Real S3 Client for:
- âœ… Production
- âœ… Multi-process apps
- âœ… Long-term persistence

## Migration Guide

### From LocalStack to MemoryClient

**Before:**
```javascript
import { S3db } from 's3db.js';

// Required: Docker, LocalStack running
const db = new S3db({
  connectionString: 'http://test:test@localhost:4566/bucket'
});
```

**After:**
```javascript
import { S3db, MemoryClient } from 's3db.js';

// Zero infrastructure!
const db = new S3db({
  client: new MemoryClient()
});
```

### From Real S3 to MemoryClient (for tests)

**Before:**
```javascript
import { S3db } from 's3db.js';

const db = new S3db({
  connectionString: process.env.BUCKET_CONNECTION_STRING
});
```

**After:**
```javascript
import { createMemoryDatabaseForTest } from './tests/config.js';

// Use helper
const db = createMemoryDatabaseForTest('my-test');
```

## Examples

See `tests/clients/memory-client.test.js` for comprehensive examples.

## Troubleshooting

### Memory Leaks in Tests

**Problem:** Tests accumulate memory over time.

**Solution:** Clear client after each test:

```javascript
afterEach(async () => {
  await database.disconnect();
  database.client.clear(); // Clear memory
});
```

### Snapshot Size Too Large

**Problem:** Snapshots are huge.

**Solution:** Only snapshot what you need:

```javascript
// Create fresh client for each test instead
beforeEach(() => {
  db = createMemoryDatabaseForTest('test');
});
```

### Tests Passing with MemoryClient but Failing with Real S3

**Problem:** Real S3 has limits that MemoryClient doesn't enforce by default.

**Solution:** Enable limit enforcement:

```javascript
const db = createMemoryDatabaseForTest('test', {
  enforceLimits: true // Catches metadata > 2KB errors
});
```

## Contributing

Found a bug? Have a feature request?

Open an issue at: https://github.com/forattini-dev/s3db.js/issues

## License

MIT - Same as s3db.js

---

**Made with â¤ï¸ for the s3db.js community**

ğŸš€ **Happy (fast) testing!**
